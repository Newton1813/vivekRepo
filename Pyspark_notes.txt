1: Basic HDFS commands:
> hadoop fs --> It will show all the available commmands in hdfs
> hadoop fs -ls
> hadoop fs -put
> hadoop fs -get
> hadoop fs -mkdir
> hadoop fs -put <local system location> <hdfs location> or hadoop fs -CopyFromLocal
> hadoop fs -get <hdfs location> <local system location> or hadoop fs -CopyToLocal
> hdfs fsck '<file location>' -files -blocks -locations
> pf -fu hdfs(user) --> It will return process related to hdfs users

2: To start pyspark shell
> pyspark --master yarn --conf spark.ui.port=37465 --num-executors=20

3: Copy dataset from remote machine to local
scp vivekyadav@gw01.itversity.com:/home/vivekyadav/data/lca C:\Users\vivek\Desktop\Python_workbench\itversity_data\input

4: Reading data from file in form of RDDs : Need to use SparkContext object (i.e sc)
from pyspark import SparkContext, SparkConf
conf = SparkConf.setAppName("testing").setMaster("local")
sc = SparkContext(conf=conf)
orders = sc.textFile("<Fully qualified path till file>")
print (type(orders)) # It will print the type of data set orders
print orders
#If we have file in hdfs then directly we can call using sc.textFile() method
#If we have file in local machine we need to take it in data structures like list and then create the RDDs ahead
ordersLocal = open("/data/retail_db/orders/part-00000").read().splitlines() # python metgods to read file
ordersRDD = sc.parallelize(ordersLocal) #created RDD from the list --> convert from linear ds to distributed ds and collect() used to convert rdd to list
ordersRDD.count()
print ordersRDD

5: Exercise for generatiing revenue from the order_items:
orderItems = sc.textFile("/public/retail_db/order_items")
for i in orderItems.take(10): print(i)

orderItemsFilter = orderItems.filter(lambda oi: int(oi.split(",")[1])==2)
orderItemsMap = orderItemsFilter.map(lambda oi: float(oi.split(",")[4]))
orderRevenue = orderItemsMap.reduce(lambda t, v: t+v)
print(orderRevenue)

6: Word count program and use of map and flatMap with lambda function:
lines = sc.textFile("file path name")
word_flatMap=lines.flatMap(lambda line:line.split(" "))
word_map=word_flatMap.map(lambda word:(word,1))
word_counts = word_map.reduceByKey(lambda t,v:t+v)
for i in word_counts.collect():print(i)

7: Use orderItems file to do below task:
#Revenue for each order_id
In sql:
select order_item_order_id,sum(order_item_subtotal) from order_items group by order_item_order_id limit 20;

In pyspark:
orderItems = sc.textFile("/public/retail_db/order_items")
for i in orderItems.take(10):print(i)

orderItemsMap = orderItems.map(lambda x:(int(x.split(",")[1]),float(x.split(",")[4])))
revenuePerItem = orderItemsMap.reduceBykey(lambda t,v:t+v)

#Get order items generating maximum quantity per order id
orderIdDetails = orderItems.map(lambda x:(int(x.split(",")[1]),x))
orderItemWithMaxQuantityPerItem = orderIdDetails.reduceByKey(lambda max, ele:max if int(max.split(",")[3]) > int(ele.split(",")[3]) else ele)
for i in orderItemWithMaxQuantityPerItem.take(10):print(i)

8: Calculate revenue and count of order irems as per order:

9: Use of reduceByKey:
orderItems = sc.textFile("/public/retail_db/order_items")
orderItemsMap = orderItems.map(lambda oi:(int(oi.split(",")[1]),(float(oi.split(",")[4]),1)))
revenuePerOrder = orderItemsMap.reduceByKey(lambda total,value:(total[0]+value[0],total[1]+value[1]))
for i in revenuePerOrder.take(10):print(i)

5: writing the data into files : Need to use SparkContext object (i.e sc)
