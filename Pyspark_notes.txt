1: Basic HDFS commands:
> hadoop fs --> It will show all the available commmands in hdfs
> hadoop fs -ls
> hadoop fs -put
> hadoop fs -get
> hadoop fs -mkdir
> hadoop fs -put <local system location> <hdfs location> or hadoop fs -CopyFromLocal
> hadoop fs -get <hdfs location> <local system location> or hadoop fs -CopyToLocal
> hdfs fsck '<file location>' -files -blocks -locations
> pf -fu hdfs(user) --> It will return process related to hdfs users

2: To start pyspark shell
> pyspark --master yarn --conf spark.ui.port=37465 --num-executors=20

3: Copy dataset from remote machine to local
scp vivekyadav@gw01.itversity.com:/home/vivekyadav/data/lca C:\Users\vivek\Desktop\Python_workbench\itversity_data\input

4: Reading data from file in form of RDDs : Need to use SparkContext object (i.e sc)
from pyspark import SparkContext, SparkConf
conf = SparkConf.setAppName("testing").setMaster("local")
sc = SparkContext(conf=conf)
orders = sc.textFile("<Fully qualified path till file>")
print (type(orders)) # It will print the type of data set orders
print orders
#If we have file in hdfs then directly we can call using sc.textFile() method
#If we have file in local machine we need to take it in data structures like list and then create the RDDs ahead
ordersLocal = open("/data/retail_db/orders/part-00000").read().splitlines() # python metgods to read file
ordersRDD = sc.parallelize(ordersLocal) #created RDD from the list --> convert from linear ds to distributed ds and collect() used to convert rdd to list
ordersRDD.count()
print ordersRDD

5: Exercise for generatiing revenue from the order_items:
orderItems = sc.textFile("/public/retail_db/order_items")
for i in orderItems.take(10): print(i)

orderItemsFilter = orderItems.filter(lambda oi: int(oi.split(",")[1])==2)
orderItemsMap = orderItemsFilter.map(lambda oi: float(oi.split(",")[4]))
orderRevenue = orderItemsMap.reduce(lambda t, v: t+v)
print(orderRevenue)

6: Word count program and use of map and flatMap with lambda function:
lines = sc.textFile("file path name")
word_flatMap=lines.flatMap(lambda line:line.split(" "))
word_map=word_flatMap.map(lambda word:(word,1))
word_counts = word_map.reduceByKey(lambda t,v:t+v)
for i in word_counts.collect():print(i)

5: writing the data into files : Need to use SparkContext object (i.e sc)
