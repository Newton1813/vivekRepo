1: Basic HDFS commands:
> hadoop fs --> It will show all the available commmands in hdfs
> hadoop fs -ls
> hadoop fs -put
> hadoop fs -get
> hadoop fs -mkdir
> hadoop fs -put <local system location> <hdfs location> or hadoop fs -CopyFromLocal
> hadoop fs -get <hdfs location> <local system location> or hadoop fs -CopyToLocal
> hdfs fsck '<file location>' -files -blocks -locations
> pf -fu hdfs(user) --> It will return process related to hdfs users

2: To start pyspark shell
> pyspark --master yarn --conf spark.ui.port=37465 --num-executors=20

3: Copy dataset from remote machine to local
scp vivekyadav@gw01.itversity.com:/home/vivekyadav/data/lca C:\Users\vivek\Desktop\Python_workbench\itversity_data\input

4: Reading data from file in form of RDDs : Need to use SparkContext object (i.e sc)
from pyspark import SparkContext, SparkConf
conf = SparkConf.setAppName("testing").setMaster("local")
sc = SparkContext(conf=conf)
orders = sc.textFile("<Fully qualified path till file>")
print (type(orders)) # It will print the type of data set orders
print orders
#If we have file in hdfs then directly we can call using sc.textFile() method
#If we have file in local machine we need to take it in data structures like list and then create the RDDs ahead
ordersLocal = open("/data/retail_db/orders/part-00000").read().splitlines() # python metgods to read file
ordersRDD = sc.parallelize(ordersLocal) #created RDD from the list --> convert from linear ds to distributed ds and collect() used to convert rdd to list
ordersRDD.count()
print ordersRDD

5: Exercise for generatiing revenue from the order_items:
orderItems = sc.textFile("/public/retail_db/order_items")
for i in orderItems.take(10): print(i)

orderItemsFilter = orderItems.filter(lambda oi: int(oi.split(",")[1])==2)
orderItemsMap = orderItemsFilter.map(lambda oi: float(oi.split(",")[4]))
orderRevenue = orderItemsMap.reduce(lambda t, v: t+v)
print(orderRevenue)

6: Word count program and use of map and flatMap with lambda function:
lines = sc.textFile("file path name")
word_flatMap=lines.flatMap(lambda line:line.split(" "))
word_map=word_flatMap.map(lambda word:(word,1))
word_counts = word_map.reduceByKey(lambda t,v:t+v)
for i in word_counts.collect():print(i)

7: Use orderItems file to do below task:
#Revenue for each order_id
In sql:
select order_item_order_id,sum(order_item_subtotal) from order_items group by order_item_order_id limit 20;

In pyspark:
orderItems = sc.textFile("/public/retail_db/order_items")
for i in orderItems.take(10):print(i)

orderItemsMap = orderItems.map(lambda x:(int(x.split(",")[1]),float(x.split(",")[4])))
revenuePerItem = orderItemsMap.reduceBykey(lambda t,v:t+v)

#Get order items generating maximum quantity per order id
orderIdDetails = orderItems.map(lambda x:(int(x.split(",")[1]),x))
orderItemWithMaxQuantityPerItem = orderIdDetails.reduceByKey(lambda max, ele:max if int(max.split(",")[3]) > int(ele.split(",")[3]) else ele)
for i in orderItemWithMaxQuantityPerItem.take(10):print(i)
 
#Calculate revenue and count of order irems as per order using reduceByKey:
8: Use of reduceByKey:
orderItems = sc.textFile("/public/retail_db/order_items")
orderItemsMap = orderItems.map(lambda oi:(int(oi.split(",")[1]),(float(oi.split(",")[4]),1)))
revenuePerOrder = orderItemsMap.reduceByKey(lambda total,value:(total[0]+value[0],total[1]+value[1]))
for i in revenuePerOrder.take(10):print(i)

#Calculate revenue and count of order irems as per order using aggregateByKey:
9: Use of aggregateByKey:
orderItems = sc.textFile("/public/retail_db/order_items")
orderItemsMap = orderItems.map(lambda oi:(int(oi.split(",")[1]),(float(oi.split(",")[4]),1)))
revenuePerOrder = orderItemsMap.aggregateByKey((0.0,0), lambda inter,revenue:(inter[0]+revenue,inter[1]+1), lambda final,inter: (final[0]+inter[0],final[1]+inter[1 ]))
revenuePerOrder.saveAsTextFile("/user/vivekyadav/revenuePerOrderAndCount")
for i in revenuePerOrder.take(10).print(i)

#Problem statement:Get daly revenue per customer : join orders,order_items and customers
#Using hive:
select c.customer_id,concat(c.customer_fname," ",c.customer_lname) as customer_name,t.revenue,t.order_date from customers c
inner join 
(select o.order_customer_id,o.order_date,sum(oi.order_item_subtotal) as revenue from orders o
inner join 
order_items oi on o.order_id=oi.order_item_order_id
group by o.order_customer_id,o.order_date) t
on c.customer_id=t.order_customer_id
limit 20;

#using pyspark:
#orders detail
inputPath = sys.argv[2]
orders = sc.textFile(inputPath+"orders")
ordersMap = orders.map(lambda o:(int(o.split(",")[0]),(o.split(",")[1],int(o.split(",")[2]))))

#customers detail
customerPath = sys.argv[3]
customers = open(customerPath).read().splitlines()
customerMap = map(lambda c:((int(c.split(",")[0]),c.split(",")[1]+" "+c.split(",")[2])),customers)
customersDict = dict(customerMap)
customersBV = sc.broadcast(customersDict)

#order_items detail
order_items = sc.textFile(inputPath+"order_items")
order_itemsMap = order_items.map(lambda oi:(int(oi.split(",")[1]),float(oi.split(",")[4])))

ordersMapJoinorder_itemsMap = ordersMap.join(order_itemsMap)

#Revenue per date per customer id
revenue = ordersMapJoinorder_itemsMap.\
    map(lambda o:((o[1][0][0], customersBV.value[o[1][0][1]]), o[1][1])).reduceByKey(lambda t,v:t+v)

revenueTabSep= revenue.map(lambda o: o[0][0]+"\t"+o[0][1]+"\t"+str(o[1]))
for i in revenueTabSep.take(10):print(i)

#run the same program to cluster
spark-submit \
    --master yarn \
    --conf spark.ui.port=55555 \
    sparkdemo/src/main/
    sparkdemo/src/main/python/retail/python_file_name.py \
    yarn-client argv1 argv2 argv3


10: To convert a RDD into collection --> by using collect()
products = sc.textFile("public/retails_db/products")
productFiltered = products.filter(lambda p:p.split(",")[4]!="")
productsList = productFiltered.collect()
productsList[:10]

11: Use of groupByKey
 productsByCategory = pythonFiltered.map(lambda p: (int(p.split(",")[1]),p)).groupByKey()
 
12: Use of flatMap to parse the grouped collection
#create a function to get top records
def getTopNProducts(productsList,topN):
    sortedProducts = sorted(productsList, key= lambda k: float(k.split(",")[4]), reverse=True)
    return sortedProducts[:topN]
    
products = sc.textFile("/public/retail_db/products")
productsFiltered = products.filter(lambda p: p.split(",")[4]!="") 

productsByCategory =  productsFiltered.map(lambda p:(int(p.split(",")[1]),p)).groupByKey()  
topNProductsbyCategory = productsByCategory.flatMap(lambda p: getTopNProducts(list(p[1]),3))